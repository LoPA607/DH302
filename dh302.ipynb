{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/DH302/blob/main/dh302.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhTF7Ii7iOzU"
      },
      "outputs": [],
      "source": [
        "import os, glob, re\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import glm\n",
        "from statsmodels.genmod.families import NegativeBinomial\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lcGEJEfxb-0"
      },
      "outputs": [],
      "source": [
        "# 0. Install required libs (run once)\n",
        "!pip install cdsapi xarray netCDF4 pandas numpy rioxarray --quiet\n",
        "\n",
        "# If you prefer GEE approach instead of CDS, tell me and I'll provide that variant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hczDta5Ny2DF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os, textwrap\n",
        "\n",
        "print(\"If you already have a ~/.cdsapirc file, skip this cell. Otherwise paste your CDS API 'key:' value when prompted.\")\n",
        "\n",
        "key = input(\"Paste your CDS API key (format 'user:id'): \").strip()\n",
        "cds_content = f\"url: https://cds.climate.copernicus.eu/api\\nkey: {key}\\n\" # Corrected API URL\n",
        "open('/root/.cdsapirc', 'w').write(cds_content)\n",
        "print(\"Wrote /root/.cdsapirc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT5z9b0ZHYOu"
      },
      "outputs": [],
      "source": [
        "import cdsapi\n",
        "c = cdsapi.Client()\n",
        "\n",
        "years = ['2011','2012','2013','2014','2015']\n",
        "\n",
        "for y in years:\n",
        "    print(\"Downloading year:\", y)\n",
        "\n",
        "    c.retrieve(\n",
        "        'reanalysis-era5-land',\n",
        "        {\n",
        "            'format': 'netcdf',\n",
        "            'variable': [\n",
        "                '2m_temperature',\n",
        "                'total_precipitation',\n",
        "                '2m_dewpoint_temperature'\n",
        "            ],\n",
        "            'year': y,\n",
        "            'month': [f\"{m:02d}\" for m in range(1,13)],\n",
        "            'day': [f\"{d:02d}\" for d in range(1,32)],\n",
        "            'time': '00:00',\n",
        "            'area': [28.8, 77.0, 28.4, 77.4],  # Delhi box\n",
        "        },\n",
        "        f'delhi_era5_daily_{y}.nc'\n",
        "    )\n",
        "\n",
        "print(\"✔ All years downloaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK8XCVnHAbaY"
      },
      "outputs": [],
      "source": [
        "\n",
        "years = ['2011','2012','2013','2014','2015']\n",
        "\n",
        "for y in years:\n",
        "    input_file = f\"/content/delhi_era5_daily_{y}.nc\"\n",
        "    output_dir = f\"era5_extracted_daily_{y}\" # Unique directory for each year\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {input_file} to {output_dir}...\")\n",
        "    with zipfile.ZipFile(input_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "    print(f\"Finished extracting for {y}\")\n",
        "\n",
        "print(\"\\n✔ All daily ERA5 files extracted to year-specific directories.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/new_delhi_dengue_data.csv\")\n",
        "\n",
        "\n",
        "df[\"outbreak_date\"] = pd.to_datetime({\n",
        "    \"year\": df[\"year\"],\n",
        "    \"month\": df[\"mon\"],\n",
        "    \"day\": df[\"day\"]\n",
        "})\n",
        "\n",
        "df_outbreaks = df[[\n",
        "    \"sno\", \"state_ut\", \"district\", \"Disease\", \"Cases\", \"Deaths\",\n",
        "    \"Latitude\", \"Longitude\", \"outbreak_date\"\n",
        "]].copy()\n",
        "\n",
        "print(\"Outbreak dates loaded:\", df_outbreaks.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLx5ZLqAPSvu"
      },
      "outputs": [],
      "source": [
        "df_outbreak=pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCIWnpv8LkyR"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/new_delhi_dengue_data.csv\")\n",
        "\n",
        "# Extract week number\n",
        "df['week_num'] = df['week_of_outbreak'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "# Compute epidemic week's Monday\n",
        "df['outbreak_date'] = df.apply(\n",
        "    lambda r: datetime.strptime(f\"{int(r['year'])}-W{int(r['week_num'])}-1\", \"%Y-W%W-%w\"),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df[['week_of_outbreak','outbreak_date']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8q4CAVdS3DB"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import glob\n",
        "\n",
        "paths = sorted(glob.glob(\"/content/era5_extracted_daily_*/data_0.nc\"))\n",
        "print(\"Found files:\", paths)\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for p in paths:\n",
        "    ds = xr.open_dataset(p)\n",
        "    df = ds.to_dataframe().reset_index()\n",
        "    dfs.append(df)\n",
        "\n",
        "climate = pd.concat(dfs, ignore_index=True)\n",
        "print(climate.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmLkWY5NiNzi"
      },
      "outputs": [],
      "source": [
        "!pip install xarray netCDF4 statsmodels --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlslJ6lsiRw-"
      },
      "outputs": [],
      "source": [
        "era_paths = sorted(glob.glob(\"/content/era5_extracted_daily_*/data_*.nc\"))\n",
        "print(\"ERA5 files found:\", len(era_paths), \"files\")\n",
        "\n",
        "dfs = []\n",
        "for p in era_paths:\n",
        "    ds = xr.open_dataset(p)\n",
        "    df = ds.to_dataframe().reset_index()\n",
        "    dfs.append(df)\n",
        "\n",
        "climate = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Auto-detect time column\n",
        "time_col = \"time\" if \"time\" in climate.columns else (\n",
        "           \"valid_time\" if \"valid_time\" in climate.columns else \"date\")\n",
        "\n",
        "climate = climate.rename(columns={time_col: \"time\"})\n",
        "climate[\"time\"] = pd.to_datetime(climate[\"time\"])\n",
        "\n",
        "numeric_cols = climate.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [\"time\"] + numeric_cols\n",
        "climate = climate[numeric_cols]\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "climate_daily = climate.groupby(\"time\").mean().reset_index()\n",
        "\n",
        "# unit conversions\n",
        "climate_daily[\"temp_C\"] = climate_daily[\"t2m\"] - 273.15\n",
        "climate_daily[\"dewpoint_C\"] = climate_daily[\"d2m\"] - 273.15\n",
        "climate_daily[\"precip_mm\"] = climate_daily[\"tp\"] * 1000.0\n",
        "\n",
        "# RH calc (Magnus)\n",
        "A = 17.625; B = 243.04\n",
        "def rh(T, Td):\n",
        "    return (np.exp((A*Td)/(B+Td)) / np.exp((A*T)/(B+T))) * 100\n",
        "\n",
        "climate_daily[\"RH\"] = rh(climate_daily[\"temp_C\"], climate_daily[\"dewpoint_C\"])\n",
        "climate_daily = climate_daily.sort_values(\"time\").reset_index(drop=True)\n",
        "print(\"Daily climate shape:\", climate_daily.shape)\n",
        "print(\"Daily time range:\", climate_daily['time'].min(), \"->\", climate_daily['time'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC7xPXzpiVgI"
      },
      "outputs": [],
      "source": [
        "outbreak = pd.read_csv(\"/content/new_delhi_dengue_data.csv\", dtype=str)\n",
        "\n",
        "outbreak[\"year\"] = pd.to_numeric(outbreak[\"year\"], errors=\"coerce\").astype(int)\n",
        "\n",
        "# Extract week number (33rd week -> 33)\n",
        "outbreak[\"week_num\"] = outbreak[\"week_of_outbreak\"].apply(lambda w: int(re.search(r\"\\d+\", str(w)).group()))\n",
        "\n",
        "def iso_week_to_monday(y, w):\n",
        "    return datetime.fromisocalendar(int(y), int(w), 1)\n",
        "\n",
        "outbreak[\"outbreak_date\"] = outbreak.apply(lambda r: iso_week_to_monday(r[\"year\"], r[\"week_num\"]), axis=1)\n",
        "\n",
        "outbreak[\"Cases\"] = pd.to_numeric(outbreak.get(\"Cases\", 0), errors=\"coerce\").fillna(0).astype(float)\n",
        "\n",
        "print(\"Outbreak rows:\", outbreak.shape[0])\n",
        "print(\"Outbreak date range:\", outbreak['outbreak_date'].min(), \"->\", outbreak['outbreak_date'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH9-oq0Picvu"
      },
      "outputs": [],
      "source": [
        "windows = []\n",
        "\n",
        "for idx, r in outbreak.iterrows():\n",
        "    start = r[\"outbreak_date\"] - timedelta(days=56)\n",
        "    end = r[\"outbreak_date\"] - timedelta(days=1)\n",
        "\n",
        "    win = climate_daily[\n",
        "        (climate_daily[\"time\"] >= start) &\n",
        "        (climate_daily[\"time\"] <= end)\n",
        "    ].copy()\n",
        "\n",
        "    win[\"outbreak_idx\"] = idx\n",
        "    win[\"outbreak_date\"] = r[\"outbreak_date\"]\n",
        "\n",
        "    for col in outbreak.columns:\n",
        "        win[col] = r[col]\n",
        "\n",
        "    windows.append(win)\n",
        "\n",
        "df = pd.concat(windows, ignore_index=True)\n",
        "\n",
        "print(\"Window dataset shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_fAfqQdiebd"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.sort_values([\"outbreak_idx\", \"time\"]).reset_index(drop=True)\n",
        "\n",
        "df[\"precip_roll3_std\"] = (\n",
        "    df.groupby(\"outbreak_idx\")[\"precip_mm\"]\n",
        "      .rolling(3, min_periods=1)\n",
        "      .std().reset_index(0, drop=True)\n",
        "      .fillna(0)\n",
        ")\n",
        "\n",
        "df[\"precip_7sum\"] = (\n",
        "    df.groupby(\"outbreak_idx\")[\"precip_mm\"]\n",
        "      .rolling(7, min_periods=1)\n",
        "      .sum().reset_index(0, drop=True)\n",
        ")\n",
        "\n",
        "median7 = df[\"precip_7sum\"].median()\n",
        "df[\"stagnation\"] = (df[\"precip_7sum\"] < median7).astype(int)\n",
        "\n",
        "# Mosquito suitability\n",
        "def temp_suit(T): return np.clip(1 - ((T - 28)/8)**2, 0, 1)\n",
        "def rain_suit(p): return np.clip(p / 30, 0, 1)\n",
        "def rh_suit(r):   return np.clip((r - 50)/50, 0, 1)\n",
        "\n",
        "df[\"temp_suit\"] = df[\"temp_C\"].apply(temp_suit)\n",
        "df[\"rain_suit\"] = df[\"precip_mm\"].apply(rain_suit)\n",
        "df[\"rh_suit\"] = df[\"RH\"].apply(rh_suit)\n",
        "\n",
        "df[\"MSI\"] = (\n",
        "    0.4*df[\"temp_suit\"] +\n",
        "    0.3*df[\"rain_suit\"] +\n",
        "    0.3*df[\"rh_suit\"]\n",
        ").clip(0,1)\n",
        "\n",
        "# Save daily window features\n",
        "df.to_csv(\"final_daily_features.csv\", index=False)\n",
        "print(\"Saved final_daily_features.csv\")\n",
        "print(df['MSI'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlbg5fWIihCx"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"outbreak_plots\", exist_ok=True)\n",
        "\n",
        "for oid in df[\"outbreak_idx\"].unique():\n",
        "    sub = df[df[\"outbreak_idx\"] == oid].sort_values(\"time\")\n",
        "    date = sub[\"outbreak_date\"].iloc[0].date()\n",
        "    base = f\"outbreak_{oid}_{date}\"\n",
        "\n",
        "    for var in [\"temp_C\", \"RH\", \"precip_mm\", \"MSI\"]:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(sub[\"time\"], sub[var])\n",
        "        plt.title(f\"{base} — {var}\")\n",
        "        plt.xlabel(\"Date\"); plt.ylabel(var)\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"outbreak_plots/{base}_{var}.png\")\n",
        "        plt.close()\n",
        "\n",
        "print(\"All outbreak plots saved → /outbreak_plots/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nww8YoKJoWM-"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------\n",
        "# 6) Weekly aggregation for causality (FIXED)\n",
        "# ----------------------------------------\n",
        "\n",
        "# Weekly MSI only from available days\n",
        "dfw = df.copy()\n",
        "dfw[\"week_start\"] = dfw[\"time\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
        "weekly_msi = dfw.groupby(\"week_start\")[\"MSI\"].mean().reset_index()\n",
        "weekly_msi = weekly_msi.rename(columns={\"week_start\":\"date\"})\n",
        "\n",
        "# Weekly dengue cases\n",
        "outbreak[\"Cases\"] = pd.to_numeric(outbreak[\"Cases\"], errors=\"coerce\").fillna(0)\n",
        "weekly_cases = outbreak[[\"outbreak_date\",\"Cases\"]].rename(columns={\"outbreak_date\":\"date\"})\n",
        "\n",
        "# Merge only overlapping weeks\n",
        "series = pd.merge(weekly_msi, weekly_cases, on=\"date\", how=\"inner\")\n",
        "series = series.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "print(\"\\nFinal weekly series:\")\n",
        "print(series.head(), \"\\n\")\n",
        "print(series.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LukEFQvo3AE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "os.makedirs(\"outbreak_summary_plots\", exist_ok=True)\n",
        "\n",
        "for oid in df[\"outbreak_idx\"].unique():\n",
        "    sub = df[df[\"outbreak_idx\"] == oid].sort_values(\"time\")\n",
        "    date = sub[\"outbreak_date\"].iloc[0].date()\n",
        "    title = f\"Outbreak {oid} — {date}\"\n",
        "\n",
        "    fig, axes = plt.subplots(4, 1, figsize=(12, 12), sharex=True)\n",
        "\n",
        "    sns.lineplot(ax=axes[0], x=sub[\"time\"], y=sub[\"temp_C\"])\n",
        "    axes[0].set_ylabel(\"Temp °C\")\n",
        "\n",
        "    sns.lineplot(ax=axes[1], x=sub[\"time\"], y=sub[\"RH\"])\n",
        "    axes[1].set_ylabel(\"RH %\")\n",
        "\n",
        "    sns.lineplot(ax=axes[2], x=sub[\"time\"], y=sub[\"precip_mm\"])\n",
        "    axes[2].set_ylabel(\"Rain (mm)\")\n",
        "\n",
        "    sns.lineplot(ax=axes[3], x=sub[\"time\"], y=sub[\"MSI\"])\n",
        "    axes[3].set_ylabel(\"MSI\")\n",
        "    axes[3].set_xlabel(\"Date\")\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig(f\"outbreak_summary_plots/outbreak_{oid}_{date}.png\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT6vTwX74aG7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df_heat = df.copy()\n",
        "df_heat[\"date\"] = df_heat[\"time\"].dt.date\n",
        "df_heat = df_heat.pivot_table(values=\"MSI\",\n",
        "                              index=df_heat[\"time\"].dt.year,\n",
        "                              columns=df_heat[\"time\"].dt.strftime(\"%m-%d\"),\n",
        "                              aggfunc=\"mean\")\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "sns.heatmap(df_heat, cmap=\"RdYlGn\", cbar_kws={\"label\": \"MSI\"})\n",
        "plt.title(\"Mosquito Suitability Heatmap\")\n",
        "plt.xlabel(\"Day of Year\")\n",
        "plt.ylabel(\"Year\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"MSI_heatmap.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Ksh3XI4fOo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df[[\"temp_C\",\"RH\",\"precip_mm\",\"MSI\",\"precip_7sum\",\"precip_roll3_std\"]].corr(),\n",
        "            annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"correlation_matrix.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbBc_hGe4i8v",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "lags = 6\n",
        "for i in range(1, lags+1):\n",
        "    plt.subplot(2,3,i)\n",
        "    plt.scatter(series[\"MSI\"], series[\"MSI\"].shift(i), s=20)\n",
        "    plt.title(f\"MSI vs MSI(lag={i})\")\n",
        "    plt.xlabel(\"MSI\")\n",
        "    plt.ylabel(f\"MSI lag {i}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lag_plots_msi.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFHSFbj94o_w"
      },
      "outputs": [],
      "source": [
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lDp0wep-kdH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUi3med_-oK6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# list of outbreaks\n",
        "outbreaks = df[\"outbreak_idx\"].unique()\n",
        "outbreaks = np.sort(outbreaks)\n",
        "\n",
        "print(\"Outbreaks detected:\", outbreaks)\n",
        "\n",
        "for oid in outbreaks:\n",
        "    sub = df[df[\"outbreak_idx\"] == oid].copy()\n",
        "\n",
        "    # select numeric feature columns\n",
        "    features = [\"temp_C\", \"RH\", \"precip_mm\", \"precip_7sum\",\n",
        "                \"precip_roll3_std\", \"MSI\"]\n",
        "\n",
        "    corr = sub[features].corr()\n",
        "\n",
        "    # plot heatmap\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    plt.title(f\"Correlation Matrix – Outbreak {oid}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # save\n",
        "    filename = f\"correlation_outbreak_{oid}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Saved: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNqW5Nz7-7x-"
      },
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load MODIS LAI\n",
        "lai = pd.read_csv(\"/content/Delhi_LAI_8day.csv\")\n",
        "\n",
        "lai['date'] = pd.to_datetime(lai['date'])\n",
        "lai = lai[['date','LAI']].sort_values('date')\n",
        "\n",
        "# DAILY interpolation\n",
        "lai_daily = lai.set_index('date').resample('D').interpolate(method='linear')\n",
        "\n",
        "lai_daily = lai_daily.reset_index()\n",
        "print(lai_daily.head(), lai_daily.tail())\n",
        "\n",
        "lai_daily.to_csv(\"Delhi_LAI_daily.csv\", index=False)\n",
        "print(\"Saved Delhi_LAI_daily.csv\")"
      ],
      "metadata": {
        "id": "5nzhI1Ob-jRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LOAD FINAL MERGED DATA\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"/content/final_with_LAI.csv\", parse_dates=[\"time\"])\n",
        "df['outbreak_date'] = pd.to_datetime(df['outbreak_date'])\n",
        "\n",
        "# -----------------------------\n",
        "# FIX LAI (missing values forward/backward fill)\n",
        "# -----------------------------\n",
        "df[\"LAI\"] = df[\"LAI\"].replace({None: np.nan})\n",
        "df[\"LAI\"] = df[\"LAI\"].astype(float)\n",
        "df[\"LAI\"] = df[\"LAI\"].interpolate().bfill().ffill()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# NORMALIZED SUITABILITY FUNCTIONS\n",
        "# -----------------------------\n",
        "def temp_suit(T):\n",
        "    return np.clip(1 - ((T - 28) / 8) ** 2, 0, 1)\n",
        "\n",
        "def rh_suit(r):\n",
        "    return np.clip((r - 50) / 50, 0, 1)\n",
        "\n",
        "def rain_suit(p):\n",
        "    return np.clip(p / 30, 0, 1)\n",
        "\n",
        "def lai_suit(l):\n",
        "    return np.clip(l / 5, 0, 1)    # LAI 0–5 typical range\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# COMPUTE SUITABILITIES\n",
        "# -----------------------------\n",
        "df[\"temp_suit\"] = df[\"temp_C\"].apply(temp_suit)\n",
        "df[\"rh_suit\"]   = df[\"RH\"].apply(rh_suit)\n",
        "df[\"rain_suit\"] = df[\"precip_mm\"].apply(rain_suit)\n",
        "df[\"lai_suit\"]  = df[\"LAI\"].apply(lai_suit)\n",
        "\n",
        "# -----------------------------\n",
        "# NEW MSI (WITH LAI WEIGHTED)\n",
        "# -----------------------------\n",
        "df[\"MSI_new\"] = (\n",
        "    0.40 * df[\"temp_suit\"] +\n",
        "    0.20 * df[\"rh_suit\"] +\n",
        "    0.20 * df[\"rain_suit\"] +\n",
        "    0.20 * df[\"lai_suit\"]\n",
        ")\n",
        "\n",
        "df[\"MSI_new\"] = df[\"MSI_new\"].clip(0, 1)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# PLOTTING FOR EACH OUTBREAK\n",
        "# ==============================\n",
        "os.makedirs(\"outbreak_plots_lai\", exist_ok=True)\n",
        "\n",
        "for oid in df[\"outbreak_idx\"].unique():\n",
        "\n",
        "    sub = df[df[\"outbreak_idx\"] == oid].sort_values(\"time\")\n",
        "    outbreak_date = sub[\"outbreak_date\"].iloc[0].date()\n",
        "\n",
        "    fig, axes = plt.subplots(5, 1, figsize=(12, 14), sharex=True)\n",
        "\n",
        "    sns.lineplot(ax=axes[0], x=sub[\"time\"], y=sub[\"temp_C\"])\n",
        "    axes[0].set_ylabel(\"Temperature (°C)\")\n",
        "\n",
        "    sns.lineplot(ax=axes[1], x=sub[\"time\"], y=sub[\"RH\"])\n",
        "    axes[1].set_ylabel(\"Relative Humidity (%)\")\n",
        "\n",
        "    sns.lineplot(ax=axes[2], x=sub[\"time\"], y=sub[\"precip_mm\"])\n",
        "    axes[2].set_ylabel(\"Rainfall (mm)\")\n",
        "\n",
        "    sns.lineplot(ax=axes[3], x=sub[\"time\"], y=sub[\"LAI\"])\n",
        "    axes[3].set_ylabel(\"LAI (vegetation index)\")\n",
        "\n",
        "    sns.lineplot(ax=axes[4], x=sub[\"time\"], y=sub[\"MSI_new\"])\n",
        "    axes[4].set_ylabel(\"MSI (with LAI)\")\n",
        "    axes[4].set_xlabel(\"Date\")\n",
        "\n",
        "    plt.suptitle(f\"Outbreak {oid} — {outbreak_date}\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(f\"outbreak_plots_lai/outbreak_{oid}_{outbreak_date}_LAI.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "print(\"All LAI-environment-MSI plots saved inside `outbreak_plots_lai/`\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bkSXsTabVzNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "INPUT_PATH = \"/content/final_with_LAI.csv\"\n",
        "OUTDIR = \"/content/outbreak_analysis_outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTDIR,\"plots_time_series\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTDIR,\"plots_lags\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTDIR,\"plots_corr\"), exist_ok=True)\n",
        "\n",
        "# Load dataframe\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "# normalize time column\n",
        "time_col = None\n",
        "for c in [\"time\",\"date\",\"datetime\",\"Date\",\"date_time\"]:\n",
        "    if c in df.columns:\n",
        "        time_col = c\n",
        "        break\n",
        "if time_col is None:\n",
        "    raise ValueError(\"No time/date column found in dataframe columns: \" + \", \".join(df.columns))\n",
        "df[time_col] = pd.to_datetime(df[time_col])\n",
        "df = df.sort_values(time_col).reset_index(drop=True)\n",
        "df.rename(columns={time_col: \"time\"}, inplace=True)\n",
        "\n",
        "# Check expected columns, try common names\n",
        "for col in [\"temp_C\",\"RH\",\"precip_mm\",\"LAI\",\"cases\",\"outbreak_idx\",\"outbreak_date\"]:\n",
        "    if col not in df.columns:\n",
        "        # attempt alternatives\n",
        "        if col==\"temp_C\":\n",
        "            for alt in [\"t2m\",\"temp\",\"temperature\",\"T2M\",\"temp_K\"]:\n",
        "                if alt in df.columns:\n",
        "                    if alt==\"temp_K\" or alt==\"t2m\" or alt==\"temp_K\":\n",
        "                        # convert K to C\n",
        "                        df[\"temp_C\"] = df[alt] - 273.15\n",
        "                    else:\n",
        "                        df[\"temp_C\"] = df[alt]\n",
        "                    break\n",
        "        if col==\"precip_mm\":\n",
        "            for alt in [\"precip_mm\",\"tp\",\"precip\",\"rain\",\"rain_mm\"]:\n",
        "                if alt in df.columns:\n",
        "                    if alt==\"tp\": df[\"precip_mm\"] = df[alt]*1000.0\n",
        "                    else: df[\"precip_mm\"] = df[alt]\n",
        "                    break\n",
        "        if col==\"RH\":\n",
        "            if \"RH\" not in df.columns and \"rh\" in df.columns:\n",
        "                df[\"RH\"] = df[\"rh\"]\n",
        "        if col==\"LAI\":\n",
        "            for alt in [\"LAI\",\"lai\",\"LAI_interp\",\"LAI_daily\"]:\n",
        "                if alt in df.columns:\n",
        "                    df[\"LAI\"] = df[alt]\n",
        "                    break\n",
        "        if col==\"cases\":\n",
        "            for alt in [\"Cases\",\"cases\",\"cases_week\",\"case_count\"]:\n",
        "                if alt in df.columns:\n",
        "                    df[\"cases\"] = df[alt]\n",
        "                    break\n",
        "        if col==\"outbreak_idx\":\n",
        "            for alt in [\"outbreak_idx\",\"sno\",\"id\"]:\n",
        "                if alt in df.columns:\n",
        "                    df[\"outbreak_idx\"] = df[alt]\n",
        "                    break\n",
        "        if col==\"outbreak_date\":\n",
        "            for alt in [\"outbreak_date\",\"date_of_outbreak\",\"outbreak_day\"]:\n",
        "                if alt in df.columns:\n",
        "                    df[\"outbreak_date\"] = pd.to_datetime(df[alt])\n",
        "                    break\n",
        "\n",
        "# Final check for necessary columns\n",
        "missing = [c for c in [\"temp_C\",\"RH\",\"precip_mm\",\"LAI\",\"outbreak_idx\",\"outbreak_date\"] if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(\"Missing required columns in input file: \" + \", \".join(missing))\n",
        "\n",
        "# Ensure numeric types\n",
        "df[\"temp_C\"] = pd.to_numeric(df[\"temp_C\"], errors=\"coerce\")\n",
        "df[\"RH\"] = pd.to_numeric(df[\"RH\"], errors=\"coerce\")\n",
        "df[\"precip_mm\"] = pd.to_numeric(df[\"precip_mm\"], errors=\"coerce\")\n",
        "df[\"LAI\"] = pd.to_numeric(df[\"LAI\"], errors=\"coerce\")\n",
        "\n",
        "# fill/ interpolate small gaps in LAI\n",
        "# Set 'time' column as index for time-weighted interpolation\n",
        "df = df.set_index('time')\n",
        "df[\"LAI\"] = df[\"LAI\"].interpolate(method=\"time\").bfill().ffill()\n",
        "df = df.reset_index() # Reset index for further operations\n",
        "\n",
        "# Suitability functions\n",
        "def temp_suit(T):\n",
        "    return np.clip(1 - ((T - 28)/8)**2, 0, 1)\n",
        "def rh_suit(r): return np.clip((r - 50)/50, 0, 1)\n",
        "def rain_suit(p): return np.clip(p/30.0, 0, 1)\n",
        "def lai_suit(l): return np.clip(l/5.0, 0, 1)\n",
        "\n",
        "df[\"temp_suit\"] = temp_suit(df[\"temp_C\"])\n",
        "df[\"rh_suit\"] = rh_suit(df[\"RH\"])\n",
        "df[\"rain_suit\"] = rain_suit(df[\"precip_mm\"])\n",
        "df[\"lai_suit\"] = lai_suit(df[\"LAI\"])\n",
        "\n",
        "# New MSI with LAI (weights: temp 0.4, RH 0.2, rain 0.2, LAI 0.2)\n",
        "df[\"MSI_new\"] = (0.40*df[\"temp_suit\"] + 0.20*df[\"rh_suit\"] + 0.20*df[\"rain_suit\"] + 0.20*df[\"lai_suit\"]).clip(0,1)\n",
        "\n",
        "# Save merged dataframe with MSI_new\n",
        "OUT_CSV = os.path.join(OUTDIR, \"final_with_LAI_MSI.csv\")\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "\n",
        "# -----------------------\n",
        "# A) Time-series plots per outbreak\n",
        "# -----------------------\n",
        "time_plots = []\n",
        "for oid in sorted(df[\"outbreak_idx\"].unique()):\n",
        "    sub = df[df[\"outbreak_idx\"]==oid].sort_values(\"time\").copy()\n",
        "    if sub.empty: continue\n",
        "    out_date = pd.to_datetime(sub[\"outbreak_date\"].iloc[0]).date()\n",
        "    fig, axes = plt.subplots(5,1, figsize=(12,14), sharex=True)\n",
        "    axes[0].plot(sub[\"time\"], sub[\"temp_C\"])\n",
        "    axes[0].set_ylabel(\"Temp (°C)\")\n",
        "    axes[1].plot(sub[\"time\"], sub[\"RH\"])\n",
        "    axes[1].set_ylabel(\"RH (%)\")\n",
        "    axes[2].plot(sub[\"time\"], sub[\"precip_mm\"])\n",
        "    axes[2].set_ylabel(\"Precip (mm)\")\n",
        "    axes[3].plot(sub[\"time\"], sub[\"LAI\"])\n",
        "    axes[3].set_ylabel(\"LAI\")\n",
        "    axes[4].plot(sub[\"time\"], sub[\"MSI_new\"])\n",
        "    axes[4].set_ylabel(\"MSI_new\")\n",
        "    axes[4].set_xlabel(\"Date\")\n",
        "    fig.suptitle(f\"Outbreak {oid} — {out_date}\")\n",
        "    fig.tight_layout(rect=[0,0,1,0.97])\n",
        "    fn = os.path.join(OUTDIR,\"plots_time_series\", f\"outbreak_{oid}_{out_date}_timeseries.png\")\n",
        "    fig.savefig(fn)\n",
        "    plt.close(fig)\n",
        "    time_plots.append(fn)\n",
        "\n",
        "# -----------------------\n",
        "# B) Lag scatter plots for each variable\n",
        "# -----------------------\n",
        "lags = [1,3,7,14]\n",
        "vars_to_plot = {\"temp_C\":\"Temperature\",\"RH\":\"Relative Humidity\",\"precip_mm\":\"Precipitation\",\"LAI\":\"LAI\",\"MSI_new\":\"MSI_new\"}\n",
        "lag_plots = []\n",
        "for var in vars_to_plot:\n",
        "    for L in lags:\n",
        "        x = df[var].shift(L)\n",
        "        y = df[var]\n",
        "        mask = (~x.isna()) & (~y.isna())\n",
        "        if mask.sum() < 10:\n",
        "            continue\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.scatter(x[mask], y[mask], s=10)\n",
        "        plt.xlabel(f\"{vars_to_plot[var]} (lag {L})\")\n",
        "        plt.ylabel(f\"{vars_to_plot[var]} (today)\")\n",
        "        plt.title(f\"{vars_to_plot[var]}: today vs lag {L}\")\n",
        "        fn = os.path.join(OUTDIR,\"plots_lags\", f\"{var}_lag{L}.png\")\n",
        "        plt.tight_layout(); plt.savefig(fn); plt.close()\n",
        "        lag_plots.append(fn)\n",
        "\n",
        "# -----------------------\n",
        "# C) Correlation matrices\n",
        "# -----------------------\n",
        "# 1) Instantaneous correlation\n",
        "corr_vars = [\"temp_C\",\"RH\",\"precip_mm\",\"LAI\",\"MSI_new\"]\n",
        "corr = df[corr_vars].corr()\n",
        "# save heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(corr, vmin=-1, vmax=1)\n",
        "plt.colorbar(label=\"Pearson r\")\n",
        "plt.xticks(range(len(corr_vars)), corr_vars, rotation=45)\n",
        "plt.yticks(range(len(corr_vars)), corr_vars)\n",
        "# annotate\n",
        "for i,j in itertools.product(range(len(corr_vars)), range(len(corr_vars))):\n",
        "    plt.text(j, i, f\"{corr.iloc[i,j]:.2f}\", ha=\"center\", va=\"center\", color=\"white\" if abs(corr.iloc[i,j])>0.5 else \"black\")\n",
        "fn = os.path.join(OUTDIR,\"plots_corr\",\"correlation_matrix.png\")\n",
        "plt.title(\"Correlation matrix\")\n",
        "plt.tight_layout(); plt.savefig(fn); plt.close()\n",
        "\n",
        "# 2) Lagged cross-correlation heatmap for MSI vs variables (lags up to 8 weeks (56 days) daily)\n",
        "maxlag = 28  # days\n",
        "lag_range = range(0, maxlag+1, 2)\n",
        "heat = pd.DataFrame(index=lag_range, columns=corr_vars)\n",
        "for L in lag_range:\n",
        "    shifted = df[\"MSI_new\"].shift(-L)  # MSI at t+L relative to predictor at t\n",
        "    for v in corr_vars:\n",
        "        heat.loc[L, v] = df[v].corr(shifted)\n",
        "heat = heat.astype(float)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(heat.T, aspect='auto', vmin=-1, vmax=1)\n",
        "plt.colorbar(label=\"Pearson r\")\n",
        "plt.yticks(range(len(corr_vars)), corr_vars)\n",
        "plt.xticks(range(len(lag_range)), [str(l) for l in lag_range], rotation=90)\n",
        "plt.xlabel(\"lag (days) for MSI = f(predictor at t)\")\n",
        "plt.title(\"Lagged correlation: predictor(t) vs MSI(t+lag)\")\n",
        "fn2 = os.path.join(OUTDIR,\"plots_corr\",\"lagged_correlation_heatmap.png\")\n",
        "plt.tight_layout(); plt.savefig(fn2); plt.close()\n",
        "\n",
        "# Save heat dataframe\n",
        "heat.to_csv(os.path.join(OUTDIR,\"lagged_correlation_values.csv\"))\n",
        "\n",
        "# -----------------------\n",
        "# D) Save outputs and small sample\n",
        "# -----------------------\n",
        "OUT_SUMMARY = {\n",
        "    \"final_csv\": OUT_CSV,\n",
        "    \"final_with_msi_csv\": os.path.join(OUTDIR,\"final_with_LAI_MSI.csv\"),\n",
        "    \"time_series_plots_count\": len(time_plots),\n",
        "    \"lag_plots_count\": len(lag_plots),\n",
        "    \"corr_plot\": fn,\n",
        "    \"lagged_corr_plot\": fn2,\n",
        "    \"plots_time_series\": time_plots[:5]\n",
        "}\n",
        "\n",
        "# show a small preview of merged dataframe\n",
        "print(\"Final sample with LAI and MSI:\")\n",
        "#print name of df\n",
        "print(df.columns.tolist())\n",
        "print(df.head(20))\n",
        "\n",
        "print(\"All outputs written to:\", OUTDIR)\n",
        "print(\"Summary:\", OUT_SUMMARY)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9wm8JLLVnpfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5caab9d8"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "corr_matrix_path = os.path.join(\"/content/outbreak_analysis_outputs/plots_corr/\", \"correlation_matrix.png\")\n",
        "\n",
        "if os.path.exists(corr_matrix_path):\n",
        "    display(Image(filename=corr_matrix_path))\n",
        "else:\n",
        "    print(f\"Plot not found: {corr_matrix_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5f8dd31e"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "lagged_corr_heatmap_path = os.path.join(\"/content/outbreak_analysis_outputs/plots_corr/\", \"lagged_correlation_heatmap.png\")\n",
        "\n",
        "if os.path.exists(lagged_corr_heatmap_path):\n",
        "    display(Image(filename=lagged_corr_heatmap_path))\n",
        "else:\n",
        "    print(f\"Plot not found: {lagged_corr_heatmap_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJWRGSe0H_Us",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "df = pd.read_csv(\"/content/outbreak_analysis_outputs/final_with_LAI_MSI.csv\", parse_dates=[\"time\"])\n",
        "df['outbreak_date'] = pd.to_datetime(df['outbreak_date'])\n",
        "\n",
        "# Compute lag in days for each outbreak\n",
        "df[\"lag_days\"] = (df[\"time\"] - df[\"outbreak_date\"]).dt.days\n",
        "\n",
        "# Keep only -56 to 0 days before outbreak\n",
        "df = df[(df[\"lag_days\"] >= -56) & (df[\"lag_days\"] <= 0)]\n",
        "\n",
        "# Ensure LAI filled\n",
        "df[\"LAI\"] = df[\"LAI\"].astype(float).interpolate().bfill().ffill()\n",
        "\n",
        "# Suitability calculations\n",
        "def temp_suit(T): return np.clip(1 - ((T - 28) / 8)**2, 0, 1)\n",
        "def rh_suit(r): return np.clip((r - 50) / 50, 0, 1)\n",
        "def rain_suit(p): return np.clip(p / 30, 0, 1)\n",
        "def lai_suit(l): return np.clip(l / 5, 0, 1)\n",
        "\n",
        "df[\"temp_suit\"] = temp_suit(df[\"temp_C\"])\n",
        "df[\"rh_suit\"]   = rh_suit(df[\"RH\"])\n",
        "df[\"rain_suit\"] = rain_suit(df[\"precip_mm\"])\n",
        "df[\"lai_suit\"]  = lai_suit(df[\"LAI\"])\n",
        "\n",
        "df[\"MSI_new\"] = (\n",
        "    0.40*df[\"temp_suit\"] +\n",
        "    0.20*df[\"rh_suit\"] +\n",
        "    0.20*df[\"rain_suit\"] +\n",
        "    0.20*df[\"lai_suit\"]\n",
        ").clip(0,1)\n",
        "\n",
        "# ================\n",
        "# Plotting\n",
        "# ================\n",
        "os.makedirs(\"aligned_outbreak_plots\", exist_ok=True)\n",
        "\n",
        "params = {\n",
        "    \"temp_C\": \"Temperature (°C)\",\n",
        "    \"RH\": \"Relative Humidity (%)\",\n",
        "    \"precip_mm\": \"Rainfall (mm)\",\n",
        "    \"LAI\": \"Leaf Area Index\",\n",
        "    \"MSI_new\": \"MSI (with LAI)\"\n",
        "}\n",
        "\n",
        "# One plot per parameter\n",
        "for col, label in params.items():\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    for oid in df[\"outbreak_idx\"].unique():\n",
        "        sub = df[df[\"outbreak_idx\"] == oid]\n",
        "        plt.plot(sub[\"lag_days\"], sub[col], label=f\"O{oid}\", linewidth=2)\n",
        "\n",
        "    plt.title(f\"{label} — aligned by outbreak day\")\n",
        "    plt.xlabel(\"Days before outbreak\")\n",
        "    plt.ylabel(label)\n",
        "    plt.axvline(0, linestyle=\"--\", linewidth=2)  # outbreak day line\n",
        "    plt.legend(title=\"Outbreak\", fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"aligned_outbreak_plots/{col}_aligned.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "print(\"All aligned plots saved in folder: aligned_outbreak_plots/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Date range:\", df['time'].min(), \"→\", df['time'].max())\n",
        "print(\"Unique outbreak dates:\", df['outbreak_date'].unique())\n",
        "print(df['outbreak_idx'].value_counts())\n"
      ],
      "metadata": {
        "id": "PYoaqWn-AgWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# FULL LSTM PIPELINE WITH ALL PLOTS\n",
        "# ====================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_curve, auc\n",
        ")\n",
        "\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ============================================================\n",
        "# 1) Load Data\n",
        "# ============================================================\n",
        "\n",
        "PATH = \"/content/outbreak_analysis_outputs/final_with_LAI_MSI.csv\"\n",
        "df = pd.read_csv(PATH)\n",
        "\n",
        "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "print(\"Original length:\", len(df))\n",
        "\n",
        "# ============================================================\n",
        "# 2) Remove Duplicates\n",
        "# ============================================================\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"time\"]).sort_values(\"time\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) Reindex to daily continuous timeline\n",
        "# ============================================================\n",
        "\n",
        "full_range = pd.date_range(df[\"time\"].min(), df[\"time\"].max(), freq=\"D\")\n",
        "df = df.set_index(\"time\").reindex(full_range)\n",
        "df.index.name = \"time\"\n",
        "df = df.ffill()\n",
        "\n",
        "print(\"Final continuous length:\", len(df))\n",
        "\n",
        "# ============================================================\n",
        "# 4) Outbreak Dates\n",
        "# ============================================================\n",
        "\n",
        "OUTBREAK_DATES = pd.to_datetime([\n",
        "    \"2013-09-16\",\"2013-09-23\",\"2013-10-07\",\n",
        "    \"2015-08-10\",\"2015-08-17\",\"2015-08-24\",\"2015-08-31\",\n",
        "    \"2015-09-07\",\"2015-09-14\",\"2015-09-21\",\"2015-09-28\",\n",
        "    \"2015-10-05\",\"2015-10-12\",\"2015-10-19\",\"2015-11-02\"\n",
        "])\n",
        "\n",
        "print(\"Total outbreak dates:\", len(OUTBREAK_DATES))\n",
        "\n",
        "# ============================================================\n",
        "# 5) Build 56-Day Windows\n",
        "# ============================================================\n",
        "\n",
        "WINDOW = 56\n",
        "pos_windows = []\n",
        "\n",
        "for d in OUTBREAK_DATES:\n",
        "    start = d - pd.Timedelta(days=WINDOW)\n",
        "    if start < df.index.min():\n",
        "        print(f\"⚠ Skipped {d} (window incomplete)\")\n",
        "        continue\n",
        "\n",
        "    window = df.loc[start:d]\n",
        "    if len(window) != WINDOW + 1:\n",
        "        print(f\"⚠ Skipped {d} (length {len(window)})\")\n",
        "        continue\n",
        "\n",
        "    win = window.copy()\n",
        "    win[\"label\"] = 1\n",
        "    win[\"outbreak_date\"] = d\n",
        "    pos_windows.append(win)\n",
        "\n",
        "print(\"Positive windows:\", len(pos_windows))\n",
        "\n",
        "# ============================================================\n",
        "# 6) Negative Windows\n",
        "# ============================================================\n",
        "\n",
        "neg_windows = []\n",
        "needed = len(pos_windows)\n",
        "non_outbreak_days = df.index.difference(OUTBREAK_DATES)\n",
        "\n",
        "for d in non_outbreak_days:\n",
        "    if len(neg_windows) >= needed:\n",
        "        break\n",
        "\n",
        "    start = d - pd.Timedelta(days=WINDOW)\n",
        "    if start < df.index.min():\n",
        "        continue\n",
        "\n",
        "    window = df.loc[start:d]\n",
        "    if len(window) != WINDOW + 1:\n",
        "        continue\n",
        "\n",
        "    win = window.copy()\n",
        "    win[\"label\"] = 0\n",
        "    win[\"outbreak_date\"] = d\n",
        "    neg_windows.append(win)\n",
        "\n",
        "print(\"Negative windows:\", len(neg_windows))\n",
        "\n",
        "# ============================================================\n",
        "# 7) Combine Dataset\n",
        "# ============================================================\n",
        "\n",
        "all_windows = pd.concat(pos_windows + neg_windows)\n",
        "all_windows = all_windows.reset_index().rename(columns={\"index\": \"time\"})\n",
        "all_windows = all_windows.sort_values([\"outbreak_date\", \"time\"])\n",
        "\n",
        "print(\"Dataset shape:\", all_windows.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 8) Build LSTM Sequences\n",
        "# ============================================================\n",
        "\n",
        "SEQ_LEN = WINDOW + 1\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for date, g in all_windows.groupby(\"outbreak_date\"):\n",
        "\n",
        "    g_num = g.select_dtypes(include=[np.number]).copy()\n",
        "    g_num = g_num.drop(columns=[\"label\"], errors=\"ignore\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(g_num)\n",
        "\n",
        "    if len(X_scaled) == SEQ_LEN:\n",
        "        X_seq.append(X_scaled)\n",
        "        y_seq.append(g[\"label\"].iloc[0])\n",
        "\n",
        "X = np.array(X_seq)\n",
        "y = np.array(y_seq)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 9) Train-Test Split\n",
        "# ============================================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, shuffle=True, random_state=42\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 10) Build LSTM Model\n",
        "# ============================================================\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, X.shape[2])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=80,\n",
        "    batch_size=16,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 11) PLOTS\n",
        "# ============================================================\n",
        "\n",
        "# ---- LOSS PLOT ----\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ---- ACCURACY PLOT ----\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train Acc\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Val Acc\")\n",
        "plt.title(\"Training Accuracy Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 12) Predictions\n",
        "# ============================================================\n",
        "\n",
        "pred_probs = model.predict(X_test)\n",
        "pred_labels = (pred_probs > 0.5).astype(int)\n",
        "\n",
        "# ---- CONFUSION MATRIX ----\n",
        "cm = confusion_matrix(y_test, pred_labels)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ---- ROC CURVE ----\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 13) Classification Report\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nCLASSIFICATION REPORT\")\n",
        "print(classification_report(y_test, pred_labels))\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(len(y_test)):\n",
        "    print(f\"True={y_test[i]}, Prob={pred_probs[i][0]:.3f}, Pred={pred_labels[i][0]}\")\n",
        "\n",
        "# ============================================================\n",
        "# 14) Save Model\n",
        "# ============================================================\n",
        "\n",
        "model.save(\"/content/lstm_outbreak_model.h5\")\n",
        "print(\"\\nModel saved as: /content/lstm_outbreak_model.h5\")\n"
      ],
      "metadata": {
        "id": "3RqRV6nzI1cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "qtjI6kE8j4b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FULL CAUSALITY ANALYSIS PIPELINE\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.stattools import grangercausalitytests, ccf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================\n",
        "# 1) LOAD DATA\n",
        "# ============================================\n",
        "PATH = \"/content/outbreak_analysis_outputs/final_with_LAI_MSI.csv\"\n",
        "\n",
        "df = pd.read_csv(PATH, parse_dates=[\"time\"])\n",
        "df = df.sort_values(\"time\").reset_index(drop=True)\n",
        "\n",
        "print(\"Loaded:\", df.shape)\n",
        "\n",
        "# outbreak_flag from outbreak_idx\n",
        "df[\"outbreak_flag\"] = (df[\"outbreak_idx\"] > 0).astype(int)\n",
        "\n",
        "# ============================================\n",
        "# 2) SMOOTH OUTBREAK SIGNAL (required for Granger)\n",
        "# ============================================\n",
        "df[\"outbreak_smooth\"] = (\n",
        "    df[\"outbreak_flag\"]\n",
        "    .rolling(window=14, center=True, min_periods=1)\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "# Plot smoothed signal\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(df[\"time\"], df[\"outbreak_smooth\"])\n",
        "plt.title(\"Smoothed Outbreak Signal\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 3) CLEAN VARIABLES FOR ANALYSIS\n",
        "# ============================================\n",
        "causal_vars = [\"temp_C\", \"RH\", \"precip_mm\", \"LAI\", \"MSI_new\"]\n",
        "\n",
        "for col in causal_vars:\n",
        "    df[col] = df[col].interpolate().bfill().ffill()\n",
        "\n",
        "# ============================================\n",
        "# 4) GRANGER CAUSALITY FUNCTION\n",
        "# ============================================\n",
        "def run_granger(df, var, maxlag=30):\n",
        "\n",
        "    print(f\"\\n\\n===== GRANGER CAUSALITY: {var} → outbreak_smooth =====\")\n",
        "\n",
        "    data = df[[\"outbreak_smooth\", var]].dropna()\n",
        "\n",
        "    result = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n",
        "\n",
        "    for lag in [1, 7, 14, 21, 30]:\n",
        "        if lag in result:\n",
        "            p = result[lag][0][\"ssr_chi2test\"][1]\n",
        "            print(f\"Lag {lag:2d} → p = {p:.5f}\")\n",
        "\n",
        "# ============================================\n",
        "# 5) RUN GRANGER FOR EACH VARIABLE\n",
        "# ============================================\n",
        "for var in causal_vars:\n",
        "    run_granger(df, var)\n",
        "\n",
        "# ============================================\n",
        "# CORRELATION HEATMAP\n",
        "# ============================================\n",
        "heat_df = df[causal_vars ]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(\n",
        "    heat_df.corr(),\n",
        "    annot=True,\n",
        "    cmap=\"coolwarm\",\n",
        "    vmin=-1,\n",
        "    vmax=1\n",
        ")\n",
        "plt.title(\"Correlation Heatmap (Environmental Factors vs Outbreak Risk)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wQPjMbtBJ-RT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP60eTiLZ0PMGC/7O8eVRoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}